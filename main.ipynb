{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7691610e-65b8-4955-8a4d-27fca9b76373",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bb06abd-6a59-4298-976d-f2cd487e9ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import dataset as d\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26806a2c-9755-45f2-b945-4cdc26dc4165",
   "metadata": {},
   "source": [
    "## Hyperparameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaffb268-9a47-45da-942b-f6b60b52b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':224,\n",
    "    'EPOCHS':10, #Your Epochs,\n",
    "    'LR':1e-5, #Your Learning Rate,\n",
    "    'BATCH_SIZE': 128, #Your Batch Size,\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b0fbf6-43b7-4d09-81da-149147f5fa44",
   "metadata": {},
   "source": [
    "## Fixed Random-Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a0e6a64-4f23-4813-9426-e0b56ce797ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcae6476-b1cc-434b-8f86-5149a283858d",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.py 의 CustomDataset 클래스 사용\n",
    "import dataset as d\n",
    "from util.preprocessing import  *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_mean = (0.42008194, 0.3838274, 0.34902292)\n",
    "train_Std = (0.23926373, 0.22593886, 0.22363442)\n",
    "\n",
    "test_mean = (0.4216005, 0.38125762, 0.34539804)\n",
    "test_Std = (0.23252015, 0.21890979, 0.21627444)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a601035f-45f0-4855-97a3-b58cf408a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/open/train.csv')\n",
    "test_data = pd.read_csv('./data/open/test.csv')\n",
    "train_transform = d.ImageTransForm(CFG['IMG_SIZE'], train_mean, train_Std)\n",
    "test_transform = d.ImageTransForm(CFG['IMG_SIZE'], test_mean, test_Std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = d.CustomDataset(train_data, 'train', transform=train_transform)\n",
    "test_dataset = d.CustomDataset(test_data, 'test', transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgs, moss, comments = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 긴 단어 탐색 (패딩을 위함)\n",
    "max_len_train = max(len(c) for c in train_data['comments'])\n",
    "max_len_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# 단어 사전 생성\n",
    "all_comments = ' '.join(train_data['comments']).split()\n",
    "vocab = set(all_comments)\n",
    "vocab = ['<PAD>', '<SOS>', '<EOS>'] + list(vocab)\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 224, 224]) torch.Size([128]) 128\n"
     ]
    }
   ],
   "source": [
    "for i, m, c in train_loader:\n",
    "    print(i.size(), m.size(), len(c))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1])\n",
      "torch.Size([128, 511])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "from train.models.encoder_resnet import EncoderResnet\n",
    "encoder = EncoderResnet(512)\n",
    "out, mos = encoder.forward(imgs)\n",
    "print(mos.shape)\n",
    "print(out.shape)\n",
    "print(type(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 / 손실함수 / 옵티마이저 생성\n",
    "from train.models.seq2seq import Seq2seq\n",
    "from common.optimizer import Adam\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "wordvec_size = 456\n",
    "hidden_size = 512\n",
    "\n",
    "model = Seq2seq(len(vocab), 456, 512)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
    "optimizer = Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbbfc72-534d-46d7-b63e-56d28a43b04e",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a36b9f43-93c5-4c1f-bb4f-e3dae2392e3d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\user\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/583 [00:20<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (128,455) and (512,2048) not aligned: 455 (dim 1) != 512 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\2023 Dacon Samsung image captioning\\main.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/2023%20Dacon%20Samsung%20image%20captioning/main.ipynb#X25sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m comments_tensor \u001b[39m=\u001b[39m pad_sequences(comments_tensor, \u001b[39m456\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/2023%20Dacon%20Samsung%20image%20captioning/main.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m comments_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(comments_tensor)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/2023%20Dacon%20Samsung%20image%20captioning/main.ipynb#X25sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(imgs, comments_tensor)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/2023%20Dacon%20Samsung%20image%20captioning/main.ipynb#X25sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m model\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/2023%20Dacon%20Samsung%20image%20captioning/main.ipynb#X25sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m optimizer\u001b[39m.\u001b[39mupdate(model\u001b[39m.\u001b[39mparams, model\u001b[39m.\u001b[39mgrads)\n",
      "File \u001b[1;32mc:\\Users\\user\\2023 Dacon Samsung image captioning\\train\\models\\seq2seq.py:21\u001b[0m, in \u001b[0;36mSeq2seq.forward\u001b[1;34m(self, xs, ts)\u001b[0m\n\u001b[0;32m     18\u001b[0m decoder_xs, decoder_ts \u001b[39m=\u001b[39m ts[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], ts[:, \u001b[39m1\u001b[39m:]\n\u001b[0;32m     20\u001b[0m h, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mforward(xs)\n\u001b[1;32m---> 21\u001b[0m score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder\u001b[39m.\u001b[39;49mforward(decoder_xs, h)\n\u001b[0;32m     22\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax\u001b[39m.\u001b[39mforward(score, decoder_ts)\n\u001b[0;32m     23\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\user\\2023 Dacon Samsung image captioning\\train\\models\\decoder_seq.py:30\u001b[0m, in \u001b[0;36mDecoderSeq.forward\u001b[1;34m(self, xs, imgfeature)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm\u001b[39m.\u001b[39mset_state(imgfeature)\n\u001b[0;32m     29\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed\u001b[39m.\u001b[39mforward(xs)\n\u001b[1;32m---> 30\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm\u001b[39m.\u001b[39;49mforward(out)\n\u001b[0;32m     31\u001b[0m score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maffine\u001b[39m.\u001b[39mforward(out)\n\u001b[0;32m     32\u001b[0m \u001b[39mreturn\u001b[39;00m score\n",
      "File \u001b[1;32mc:\\Users\\user\\2023 Dacon Samsung image captioning\\common\\time_layers.py:193\u001b[0m, in \u001b[0;36mTimeLSTM.forward\u001b[1;34m(self, xs)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(T):\n\u001b[0;32m    192\u001b[0m     layer \u001b[39m=\u001b[39m LSTM(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m--> 193\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mforward(xs[:, t, :], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mh, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc)\n\u001b[0;32m    194\u001b[0m     hs[:, t, :] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh\n\u001b[0;32m    196\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mappend(layer)\n",
      "File \u001b[1;32mc:\\Users\\user\\2023 Dacon Samsung image captioning\\common\\time_layers.py:114\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, x, h_prev, c_prev)\u001b[0m\n\u001b[0;32m    111\u001b[0m Wx, Wh, b \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams\n\u001b[0;32m    112\u001b[0m N, H \u001b[39m=\u001b[39m h_prev\u001b[39m.\u001b[39mshape\n\u001b[1;32m--> 114\u001b[0m A \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(x, Wx) \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39;49mdot(h_prev, Wh) \u001b[39m+\u001b[39m b\n\u001b[0;32m    116\u001b[0m f \u001b[39m=\u001b[39m A[:, :H]\n\u001b[0;32m    117\u001b[0m g \u001b[39m=\u001b[39m A[:, H:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mH]\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (128,455) and (512,2048) not aligned: 455 (dim 1) != 512 (dim 0)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# 학습\n",
    "total_loss = 0\n",
    "\n",
    "for epoch in range(CFG['EPOCHS']):\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for imgs, moss, comments in loop:\n",
    "        imgs = imgs.float()\n",
    "        \n",
    "        # Batch Preprocessing\n",
    "        comments_tensor = torch.zeros((len(comments), len(max(comments, key=len)))).long()        \n",
    "        \n",
    "        for i, comment in enumerate(comments):\n",
    "            tokenized = ['<SOS>'] + comment.split() + ['<EOS>']\n",
    "            comments_tensor[i, :len(tokenized)] = torch.tensor([word2idx[word] for word in tokenized])\n",
    "        \n",
    "        comments_tensor = pad_sequences(comments_tensor, 456, padding='post')\n",
    "        comments_tensor = torch.tensor(comments_tensor)\n",
    "        \n",
    "        loss = model.forward(imgs, comments_tensor)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        total_loss += loss\n",
    "\n",
    "\n",
    "        \n",
    "        start_id = comments_tensor[0]\n",
    "        correct = comments_tensor[1:]\n",
    "        predicted_comments = model.generate(imgs, start_id, len(correct))\n",
    "\n",
    "        predicted_comments = ''.join([idx2word[int(c)] for c in predicted_comments])\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864072e8-85dc-435d-9198-5b9e1f61bd24",
   "metadata": {},
   "source": [
    "## Inference & Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e881eb92-a172-479e-92e2-38f852a488e5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\2023 Dacon Samsung image captioning\\main.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/2023%20Dacon%20Samsung%20image%20captioning/main.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m./data/open/test.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/2023%20Dacon%20Samsung%20image%20captioning/main.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_dataset \u001b[39m=\u001b[39m d\u001b[39m.\u001b[39mCustomDataset(test_data, transform)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/2023%20Dacon%20Samsung%20image%20captioning/main.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m test_loader \u001b[39m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/2023%20Dacon%20Samsung%20image%20captioning/main.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transform' is not defined"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('./data/open/test.csv')\n",
    "test_dataset = d.CustomDataset(test_data, transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "predicted_mos_list = []\n",
    "predicted_comments_list = []\n",
    "\n",
    "def greedy_decode(model, image, max_length=50):\n",
    "    image = image.unsqueeze(0)\n",
    "    mos, _ = model(image)\n",
    "    output_sentence = []\n",
    "    \n",
    "    # 시작 토큰 설정\n",
    "    current_token = torch.tensor([word2idx['<SOS>']])\n",
    "    hidden = None\n",
    "    features = model.cnn(image).view(image.size(0), -1)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        embeddings = model.embedding(current_token).unsqueeze(0)\n",
    "        combined = torch.cat([features.unsqueeze(1), embeddings], dim=2)\n",
    "        out, hidden = model.lstm(combined, hidden)\n",
    "        \n",
    "        output = model.fc(out.squeeze(0))\n",
    "        _, current_token = torch.max(output, dim=1)\n",
    "\n",
    "        # <EOS> 토큰에 도달하면 멈춤\n",
    "        if current_token.item() == word2idx['<EOS>']:\n",
    "            break\n",
    "\n",
    "        # <SOS> 또는 <PAD> 토큰은 생성한 캡션에 추가하지 않음\n",
    "        if current_token.item() not in [word2idx['<SOS>'], word2idx['<PAD>']]:\n",
    "            output_sentence.append(idx2word[current_token.item()])\n",
    "     \n",
    "    return mos.item(), ' '.join(output_sentence)\n",
    "\n",
    "# 추론 과정\n",
    "with torch.no_grad():\n",
    "    for imgs, _, _ in tqdm(test_loader):\n",
    "        for img in imgs:\n",
    "            img = img.float()\n",
    "            mos, caption = greedy_decode(model, img)\n",
    "            predicted_mos_list.append(mos)\n",
    "            predicted_comments_list.append(caption)\n",
    "\n",
    "# 결과 저장\n",
    "result_df = pd.DataFrame({\n",
    "    'img_name': test_data['img_name'],\n",
    "    'mos': predicted_mos_list,\n",
    "    'comments': predicted_comments_list  # 캡션 부분은 위에서 생성한 것을 사용\n",
    "})\n",
    "\n",
    "# 예측 결과에 NaN이 있다면, 제출 시 오류가 발생하므로 후처리 진행 (sample_submission.csv과 동일하게)\n",
    "result_df['comments'] = result_df['comments'].fillna('Nice Image.')\n",
    "result_df.to_csv('submit.csv', index=False)\n",
    "\n",
    "print(\"Inference completed and results saved to submit.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.14 (NGC 22.12/Python 3.8) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
