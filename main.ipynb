{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7691610e-65b8-4955-8a4d-27fca9b76373",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bb06abd-6a59-4298-976d-f2cd487e9ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import dataset as d\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26806a2c-9755-45f2-b945-4cdc26dc4165",
   "metadata": {},
   "source": [
    "## Hyperparameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaffb268-9a47-45da-942b-f6b60b52b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':224,\n",
    "    'EPOCHS':10, #Your Epochs,\n",
    "    'LR':1e-5, #Your Learning Rate,\n",
    "    'BATCH_SIZE': 128, #Your Batch Size,\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b0fbf6-43b7-4d09-81da-149147f5fa44",
   "metadata": {},
   "source": [
    "## Fixed Random-Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a0e6a64-4f23-4813-9426-e0b56ce797ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcae6476-b1cc-434b-8f86-5149a283858d",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.py 의 CustomDataset 클래스 사용\n",
    "import dataset as d\n",
    "from util.preprocessing import  *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_mean = (0.42008194, 0.3838274, 0.34902292)\n",
    "train_Std = (0.23926373, 0.22593886, 0.22363442)\n",
    "\n",
    "test_mean = (0.4216005, 0.38125762, 0.34539804)\n",
    "test_Std = (0.23252015, 0.21890979, 0.21627444)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a601035f-45f0-4855-97a3-b58cf408a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/open/train.csv')\n",
    "test_data = pd.read_csv('./data/open/test.csv')\n",
    "train_transform = d.ImageTransForm(CFG['IMG_SIZE'], train_mean, train_Std)\n",
    "test_transform = d.ImageTransForm(CFG['IMG_SIZE'], test_mean, test_Std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "train_dataset = d.CustomDataset(train_data, 'train', transform=train_transform)\n",
    "test_dataset = d.CustomDataset(test_data, 'test', transform=test_transform)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "dataset = next(iter(train_loader))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[-1.3951, -1.3951, -1.3788,  ..., -1.5754, -1.5918, -1.6082],\n          [-1.3788, -1.3951, -1.3951,  ..., -1.5427, -1.5590, -1.5590],\n          [-1.3788, -1.3788, -1.3951,  ..., -1.5263, -1.5263, -1.5263],\n          ...,\n          [-1.7393, -1.7229, -1.7229,  ..., -1.7557, -1.7557, -1.7557],\n          [-1.7229, -1.7229, -1.7229,  ..., -1.7557, -1.7557, -1.7557],\n          [-1.7229, -1.7229, -1.7229,  ..., -1.7557, -1.7557, -1.7557]],\n\n         [[-1.3517, -1.3517, -1.3343,  ..., -1.4905, -1.5252, -1.5252],\n          [-1.3170, -1.3170, -1.3170,  ..., -1.4732, -1.4905, -1.5079],\n          [-1.3170, -1.3343, -1.3343,  ..., -1.4385, -1.4385, -1.4558],\n          ...,\n          [-1.6815, -1.6641, -1.6641,  ..., -1.6988, -1.6988, -1.6988],\n          [-1.6641, -1.6641, -1.6641,  ..., -1.6988, -1.6988, -1.6988],\n          [-1.6641, -1.6641, -1.6641,  ..., -1.6988, -1.6988, -1.6988]],\n\n         [[-1.3327, -1.3327, -1.3152,  ..., -1.4555, -1.4555, -1.4730],\n          [-1.2977, -1.3152, -1.2801,  ..., -1.4555, -1.4555, -1.4379],\n          [-1.3152, -1.2977, -1.2977,  ..., -1.4029, -1.4029, -1.4029],\n          ...,\n          [-1.5256, -1.5081, -1.4905,  ..., -1.5607, -1.5607, -1.5607],\n          [-1.4905, -1.4905, -1.4905,  ..., -1.5607, -1.5607, -1.5607],\n          [-1.4905, -1.4905, -1.4905,  ..., -1.5607, -1.5607, -1.5607]]],\n\n\n        [[[ 1.0962,  0.9159,  0.9323,  ..., -0.9198, -0.7068, -0.6576],\n          [ 1.1125,  1.1453,  1.0470,  ..., -0.7395, -0.7395, -0.8543],\n          [ 1.1289,  1.1781,  1.2273,  ..., -0.5265, -0.7723, -0.7887],\n          ...,\n          [-0.1331, -0.1331, -0.1331,  ..., -1.1165, -1.0346, -1.1001],\n          [-0.2314, -0.3626, -0.2970,  ..., -1.1001, -1.0182, -1.0346],\n          [-0.1331, -0.3134, -0.4281,  ..., -1.1165, -0.9854, -1.0182]],\n\n         [[ 1.3213,  1.0262,  0.9741,  ..., -1.0913, -1.0393, -0.9178],\n          [ 1.4428,  1.4428,  1.1130,  ..., -0.8310, -0.9698, -1.0740],\n          [ 1.4601,  1.5295,  1.3560,  ..., -0.5706, -0.9178, -0.9525],\n          ...,\n          [ 0.1063,  0.1410,  0.1063,  ..., -0.9178, -0.8830, -0.9351],\n          [ 0.0369, -0.0326, -0.1367,  ..., -0.9351, -0.8830, -0.8657],\n          [ 0.0195, -0.1714, -0.2061,  ..., -0.9872, -0.8483, -0.8310]],\n\n         [[ 1.2275,  0.6663,  0.5611,  ..., -1.2801, -1.2100, -1.1749],\n          [ 1.5607,  1.3152,  0.7540,  ..., -1.0346, -1.1398, -1.3327],\n          [ 1.4554,  1.4730,  1.1223,  ..., -0.8067, -1.1048, -1.2450],\n          ...,\n          [-1.5256, -1.5431, -1.4730,  ..., -1.5607, -1.5256, -1.5607],\n          [-1.4555, -1.4905, -1.5256,  ..., -1.5607, -1.5081, -1.5256],\n          [-1.4555, -1.5081, -1.4730,  ..., -1.5607, -1.4905, -1.5081]]],\n\n\n        [[[-0.8215, -1.1329, -1.2476,  ..., -1.7393, -1.7393, -1.7393],\n          [-0.8543, -1.1657, -1.1329,  ..., -1.7393, -1.7393, -1.7557],\n          [-0.9198, -1.2149, -1.1329,  ..., -1.7557, -1.7229, -1.7393],\n          ...,\n          [-1.6902, -1.6738, -1.6738,  ..., -1.7557, -1.7557, -1.7557],\n          [-1.6902, -1.6902, -1.6902,  ..., -1.7393, -1.7557, -1.7557],\n          [-1.6902, -1.6902, -1.6738,  ..., -1.7393, -1.7229, -1.7066]],\n\n         [[-0.7095, -1.0393, -1.1608,  ..., -1.6815, -1.6815, -1.6815],\n          [-0.7442, -1.0740, -1.0393,  ..., -1.6815, -1.6815, -1.6988],\n          [-0.8136, -1.1260, -1.0393,  ..., -1.6988, -1.6641, -1.6815],\n          ...,\n          [-1.6294, -1.6120, -1.6120,  ..., -1.6988, -1.6988, -1.6988],\n          [-1.6294, -1.6294, -1.6294,  ..., -1.6815, -1.6988, -1.6988],\n          [-1.6294, -1.6294, -1.6120,  ..., -1.6815, -1.6641, -1.6467]],\n\n         [[-0.5612, -0.8943, -1.0171,  ..., -1.5431, -1.5431, -1.5431],\n          [-0.5962, -0.9294, -0.8943,  ..., -1.5431, -1.5431, -1.5607],\n          [-0.6664, -0.9820, -0.8943,  ..., -1.5607, -1.5256, -1.5431],\n          ...,\n          [-1.4905, -1.4730, -1.4730,  ..., -1.5607, -1.5607, -1.5607],\n          [-1.4905, -1.4905, -1.4905,  ..., -1.5431, -1.5607, -1.5607],\n          [-1.4905, -1.4905, -1.4730,  ..., -1.5431, -1.5256, -1.5081]]],\n\n\n        ...,\n\n\n        [[[-0.7723, -0.7887, -0.7723,  ..., -0.8215, -0.7887, -0.7068],\n          [-0.7559, -0.7887, -0.7723,  ..., -0.8215, -0.7887, -0.7068],\n          [-0.7395, -0.7559, -0.7395,  ..., -0.8379, -0.7887, -0.6904],\n          ...,\n          [ 1.7682,  1.7682,  1.8337,  ...,  0.8667,  0.8339,  0.8175],\n          [ 1.7845,  1.7518,  1.6698,  ...,  0.8339,  0.8339,  0.8339],\n          [ 1.7190,  1.7190,  1.7518,  ...,  0.8011,  0.8175,  0.8339]],\n\n         [[ 0.6617,  0.6791,  0.6791,  ...,  0.6791,  0.7138,  0.7485],\n          [ 0.6791,  0.6791,  0.6791,  ...,  0.6791,  0.7138,  0.7485],\n          [ 0.7138,  0.7138,  0.7138,  ...,  0.6791,  0.7138,  0.7659],\n          ...,\n          [ 1.3213,  1.3386,  1.4601,  ...,  0.3146,  0.3319,  0.3319],\n          [ 1.3213,  1.3213,  1.2692,  ...,  0.3146,  0.3146,  0.3146],\n          [ 1.2518,  1.1824,  1.1824,  ...,  0.3146,  0.3146,  0.3146]],\n\n         [[ 2.4550,  2.4550,  2.4550,  ...,  2.4725,  2.5076,  2.5251],\n          [ 2.4725,  2.4550,  2.4550,  ...,  2.4725,  2.4900,  2.5076],\n          [ 2.4900,  2.4550,  2.4725,  ...,  2.4550,  2.4900,  2.5076],\n          ...,\n          [ 1.4204,  1.3853,  1.4554,  ..., -1.0346, -0.9995, -0.9995],\n          [ 1.3678,  1.3678,  1.2625,  ..., -1.0522, -1.0171, -0.9995],\n          [ 1.2625,  1.2450,  1.2625,  ..., -1.0697, -1.0522, -1.0171]]],\n\n\n        [[[-1.7557, -1.7557, -1.7557,  ..., -1.7393, -1.7393, -1.7393],\n          [-1.7557, -1.7557, -1.7557,  ..., -1.7557, -1.7393, -1.7393],\n          [-1.7557, -1.7557, -1.7557,  ..., -1.7557, -1.7557, -1.7557],\n          ...,\n          [-1.7557, -1.7557, -1.7557,  ..., -1.7557, -1.7557, -1.7557],\n          [-1.7557, -1.7557, -1.7557,  ..., -1.7557, -1.7557, -1.7557],\n          [-1.7557, -1.7557, -1.7557,  ..., -1.7557, -1.7557, -1.7557]],\n\n         [[-1.6988, -1.6988, -1.6988,  ..., -1.6641, -1.6641, -1.6641],\n          [-1.6988, -1.6988, -1.6988,  ..., -1.6467, -1.6467, -1.6641],\n          [-1.6988, -1.6988, -1.6988,  ..., -1.6294, -1.6294, -1.6467],\n          ...,\n          [-1.6988, -1.6988, -1.6988,  ..., -1.6988, -1.6988, -1.6988],\n          [-1.6988, -1.6988, -1.6988,  ..., -1.6988, -1.6988, -1.6988],\n          [-1.6988, -1.6988, -1.6988,  ..., -1.6988, -1.6988, -1.6988]],\n\n         [[-1.5607, -1.5607, -1.5607,  ..., -1.4555, -1.4555, -1.4555],\n          [-1.5607, -1.5607, -1.5607,  ..., -1.4555, -1.4555, -1.4555],\n          [-1.5607, -1.5607, -1.5607,  ..., -1.4379, -1.4379, -1.4555],\n          ...,\n          [-1.5607, -1.5607, -1.5607,  ..., -1.5431, -1.5607, -1.5607],\n          [-1.5607, -1.5607, -1.5607,  ..., -1.5607, -1.5607, -1.5607],\n          [-1.5607, -1.5607, -1.5607,  ..., -1.5607, -1.5607, -1.5607]]],\n\n\n        [[[-1.7557, -1.7557, -1.7557,  ..., -1.7557, -1.7557, -1.7557],\n          [-1.7557, -1.7557, -1.7557,  ..., -1.7557, -1.7557, -1.7557],\n          [-1.7557, -1.7557, -1.7557,  ..., -1.7557, -1.7557, -1.7557],\n          ...,\n          [-1.7393, -1.7393, -1.7393,  ..., -1.7066, -1.7066, -1.7066],\n          [-1.7229, -1.7393, -1.7393,  ..., -1.6902, -1.7066, -1.7066],\n          [-1.7393, -1.7393, -1.7393,  ..., -1.7066, -1.7066, -1.7066]],\n\n         [[-1.6988, -1.6988, -1.6988,  ..., -1.6988, -1.6988, -1.6988],\n          [-1.6988, -1.6988, -1.6988,  ..., -1.6988, -1.6988, -1.6988],\n          [-1.6988, -1.6988, -1.6988,  ..., -1.6988, -1.6988, -1.6988],\n          ...,\n          [-1.6815, -1.6815, -1.6815,  ..., -1.6467, -1.6467, -1.6467],\n          [-1.6641, -1.6815, -1.6815,  ..., -1.6294, -1.6467, -1.6467],\n          [-1.6815, -1.6815, -1.6815,  ..., -1.6467, -1.6467, -1.6467]],\n\n         [[-1.5607, -1.5607, -1.5607,  ..., -1.5607, -1.5607, -1.5607],\n          [-1.5607, -1.5607, -1.5607,  ..., -1.5607, -1.5607, -1.5607],\n          [-1.5607, -1.5607, -1.5607,  ..., -1.5607, -1.5607, -1.5607],\n          ...,\n          [-1.5431, -1.5431, -1.5431,  ..., -1.5081, -1.5081, -1.5081],\n          [-1.5256, -1.5431, -1.5431,  ..., -1.4905, -1.5081, -1.5081],\n          [-1.5431, -1.5431, -1.5431,  ..., -1.5081, -1.5081, -1.5081]]]])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1139],\n",
      "        [ 0.0517],\n",
      "        [ 0.0811],\n",
      "        [ 0.0640],\n",
      "        [ 0.0299],\n",
      "        [ 0.1850],\n",
      "        [ 0.0320],\n",
      "        [ 0.0682],\n",
      "        [ 0.0551],\n",
      "        [-0.0179],\n",
      "        [ 0.0562],\n",
      "        [-0.0944],\n",
      "        [ 0.0870],\n",
      "        [-0.0128],\n",
      "        [ 0.1189],\n",
      "        [ 0.0353],\n",
      "        [ 0.0257],\n",
      "        [-0.0879],\n",
      "        [-0.1628],\n",
      "        [ 0.0047],\n",
      "        [ 0.0259],\n",
      "        [ 0.0541],\n",
      "        [ 0.2599],\n",
      "        [ 0.1204],\n",
      "        [ 0.0834],\n",
      "        [ 0.1058],\n",
      "        [ 0.0542],\n",
      "        [ 0.0924],\n",
      "        [ 0.1575],\n",
      "        [-0.0302],\n",
      "        [-0.0582],\n",
      "        [ 0.2505],\n",
      "        [ 0.0601],\n",
      "        [ 0.1939],\n",
      "        [-0.0146],\n",
      "        [-0.0400],\n",
      "        [-0.1018],\n",
      "        [ 0.2074],\n",
      "        [ 0.0834],\n",
      "        [ 0.1160],\n",
      "        [ 0.1318],\n",
      "        [-0.1284],\n",
      "        [ 0.0410],\n",
      "        [ 0.0646],\n",
      "        [ 0.0042],\n",
      "        [-0.0492],\n",
      "        [ 0.0659],\n",
      "        [ 0.1385],\n",
      "        [ 0.1579],\n",
      "        [ 0.0955],\n",
      "        [-0.0440],\n",
      "        [ 0.1382],\n",
      "        [ 0.0887],\n",
      "        [ 0.1651],\n",
      "        [ 0.1573],\n",
      "        [ 0.1819],\n",
      "        [-0.0186],\n",
      "        [ 0.3081],\n",
      "        [ 0.1765],\n",
      "        [ 0.0506],\n",
      "        [ 0.1877],\n",
      "        [-0.0109],\n",
      "        [ 0.0664],\n",
      "        [ 0.1674],\n",
      "        [-0.2545],\n",
      "        [-0.0502],\n",
      "        [ 0.1471],\n",
      "        [ 0.0566],\n",
      "        [ 0.0193],\n",
      "        [ 0.0821],\n",
      "        [ 0.0845],\n",
      "        [ 0.1044],\n",
      "        [-0.0294],\n",
      "        [ 0.0534],\n",
      "        [ 0.0900],\n",
      "        [ 0.1187],\n",
      "        [-0.0841],\n",
      "        [ 0.0847],\n",
      "        [ 0.0442],\n",
      "        [ 0.1346],\n",
      "        [ 0.0101],\n",
      "        [ 0.0196],\n",
      "        [ 0.0879],\n",
      "        [-0.0704],\n",
      "        [ 0.0642],\n",
      "        [ 0.0331],\n",
      "        [ 0.0792],\n",
      "        [-0.0343],\n",
      "        [ 0.0308],\n",
      "        [ 0.0814],\n",
      "        [ 0.2134],\n",
      "        [ 0.0301],\n",
      "        [-0.0712],\n",
      "        [ 0.0225],\n",
      "        [ 0.1214],\n",
      "        [ 0.1576],\n",
      "        [ 0.0316],\n",
      "        [ 0.1008],\n",
      "        [ 0.0387],\n",
      "        [ 0.0581],\n",
      "        [ 0.0939],\n",
      "        [ 0.0820],\n",
      "        [ 0.1112],\n",
      "        [ 0.0304],\n",
      "        [ 0.0127],\n",
      "        [-0.0777],\n",
      "        [-0.1940],\n",
      "        [ 0.1235],\n",
      "        [ 0.0903],\n",
      "        [ 0.1059],\n",
      "        [ 0.0225],\n",
      "        [ 0.1502],\n",
      "        [-0.0270],\n",
      "        [ 0.0586],\n",
      "        [ 0.1130],\n",
      "        [ 0.0876],\n",
      "        [-0.0609],\n",
      "        [ 0.0346],\n",
      "        [ 0.0591],\n",
      "        [ 0.1895],\n",
      "        [ 0.0308],\n",
      "        [ 0.0481],\n",
      "        [ 0.0169],\n",
      "        [-0.0345],\n",
      "        [ 0.0367],\n",
      "        [ 0.0359],\n",
      "        [-0.0236],\n",
      "        [ 0.0260]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "from train.models.encoder_resnet import EncoderResnet\n",
    "encoder = EncoderResnet(512)\n",
    "out, mos = encoder(dataset[0])\n",
    "print(mos.shape)\n",
    "print(out.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (401381389.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[17], line 2\u001B[0;36m\u001B[0m\n\u001B[0;31m    from train.models.encoder-resnet import EncoderResnet\u001B[0m\n\u001B[0m                             ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 단어 사전 생성\n",
    "all_comments = ' '.join(train_data['comments']).split()\n",
    "vocab = set(all_comments)\n",
    "vocab = ['<PAD>', '<SOS>', '<EOS>'] + list(vocab)\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "\n",
    "# 모델, 손실함수, 옵티마이저\n",
    "model = encoder.EncoderCNN(len(vocab))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CFG['LR'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "dfbbfc72-534d-46d7-b63e-56d28a43b04e",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a36b9f43-93c5-4c1f-bb4f-e3dae2392e3d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/583 [00:00<07:21,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 290])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/583 [00:01<06:59,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 225])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/583 [00:02<06:50,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 223])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/583 [00:02<06:46,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 245])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/583 [00:03<06:56,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 174])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/583 [00:04<06:51,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 185])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/583 [00:04<07:23,  1.30it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32mc:\\Users\\user\\2023 Dacon Samsung image captioning\\main.ipynb Cell 12\u001B[0m line \u001B[0;36m3\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/2023%20Dacon%20Samsung%20image%20captioning/main.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001B[0m total_loss \u001B[39m=\u001B[39m \u001B[39m0\u001B[39m\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/2023%20Dacon%20Samsung%20image%20captioning/main.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001B[0m loop \u001B[39m=\u001B[39m tqdm(train_loader, leave\u001B[39m=\u001B[39m\u001B[39mTrue\u001B[39;00m)\n\u001B[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/2023%20Dacon%20Samsung%20image%20captioning/main.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001B[0m \u001B[39mfor\u001B[39;00m imgs, mos, comments \u001B[39min\u001B[39;00m loop:\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/2023%20Dacon%20Samsung%20image%20captioning/main.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001B[0m     imgs, mos \u001B[39m=\u001B[39m imgs\u001B[39m.\u001B[39mfloat(), mos\u001B[39m.\u001B[39mfloat()\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/2023%20Dacon%20Samsung%20image%20captioning/main.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001B[0m     \u001B[39m# Batch Preprocessing\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1178\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1175\u001B[0m time \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_time\n\u001B[0;32m   1177\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m-> 1178\u001B[0m     \u001B[39mfor\u001B[39;00m obj \u001B[39min\u001B[39;00m iterable:\n\u001B[0;32m   1179\u001B[0m         \u001B[39myield\u001B[39;00m obj\n\u001B[0;32m   1180\u001B[0m         \u001B[39m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[0;32m   1181\u001B[0m         \u001B[39m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    627\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_sampler_iter \u001B[39mis\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m     \u001B[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    629\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_reset()  \u001B[39m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m data \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_next_data()\n\u001B[0;32m    631\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_num_yielded \u001B[39m+\u001B[39m\u001B[39m=\u001B[39m \u001B[39m1\u001B[39m\n\u001B[0;32m    632\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_dataset_kind \u001B[39m==\u001B[39m _DatasetKind\u001B[39m.\u001B[39mIterable \u001B[39mand\u001B[39;00m \\\n\u001B[0;32m    633\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_IterableDataset_len_called \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m \u001B[39mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_num_yielded \u001B[39m>\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    672\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_next_data\u001B[39m(\u001B[39mself\u001B[39m):\n\u001B[0;32m    673\u001B[0m     index \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_next_index()  \u001B[39m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 674\u001B[0m     data \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_dataset_fetcher\u001B[39m.\u001B[39mfetch(index)  \u001B[39m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    675\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_pin_memory:\n\u001B[0;32m    676\u001B[0m         data \u001B[39m=\u001B[39m _utils\u001B[39m.\u001B[39mpin_memory\u001B[39m.\u001B[39mpin_memory(data, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdataset\u001B[39m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[39m=\u001B[39m [\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdataset[idx] \u001B[39mfor\u001B[39;00m idx \u001B[39min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdataset\u001B[39m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[39m=\u001B[39m [\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdataset[idx] \u001B[39mfor\u001B[39;00m idx \u001B[39min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mc:\\Users\\user\\2023 Dacon Samsung image captioning\\dataset.py:23\u001B[0m, in \u001B[0;36mCustomDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m__getitem__\u001B[39m(\u001B[39mself\u001B[39m, idx):\n\u001B[0;32m     21\u001B[0m     img_path : \u001B[39mstr\u001B[39m\u001B[39m=\u001B[39m \u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m{\u001B[39;00mpath\u001B[39m}\u001B[39;00m\u001B[39m/data/open\u001B[39m\u001B[39m{\u001B[39;00m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdataframe\u001B[39m.\u001B[39miloc[idx][\u001B[39m'\u001B[39m\u001B[39mpath\u001B[39m\u001B[39m'\u001B[39m]\u001B[39m}\u001B[39;00m\u001B[39m.\u001B[39m\u001B[39m{\u001B[39;00m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdataframe\u001B[39m.\u001B[39miloc[idx][\u001B[39m'\u001B[39m\u001B[39mextension\u001B[39m\u001B[39m'\u001B[39m]\u001B[39m}\u001B[39;00m\u001B[39m\"\u001B[39m\n\u001B[1;32m---> 23\u001B[0m     img \u001B[39m=\u001B[39m Image\u001B[39m.\u001B[39mopen(img_path)\u001B[39m.\u001B[39mconvert(\u001B[39m'\u001B[39m\u001B[39mRGB\u001B[39m\u001B[39m'\u001B[39m)\n\u001B[0;32m     25\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mtransform:\n\u001B[0;32m     26\u001B[0m         img \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mtransform(img)\n",
      "File \u001B[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3227\u001B[0m, in \u001B[0;36mopen\u001B[1;34m(fp, mode, formats)\u001B[0m\n\u001B[0;32m   3224\u001B[0m     filename \u001B[39m=\u001B[39m fp\n\u001B[0;32m   3226\u001B[0m \u001B[39mif\u001B[39;00m filename:\n\u001B[1;32m-> 3227\u001B[0m     fp \u001B[39m=\u001B[39m builtins\u001B[39m.\u001B[39mopen(filename, \u001B[39m\"\u001B[39m\u001B[39mrb\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[0;32m   3228\u001B[0m     exclusive_fp \u001B[39m=\u001B[39m \u001B[39mTrue\u001B[39;00m\n\u001B[0;32m   3230\u001B[0m \u001B[39mtry\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 학습\n",
    "model.train()\n",
    "for epoch in range(CFG['EPOCHS']):\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for imgs, comments in loop:\n",
    "        imgs = imgs.float()\n",
    "        \n",
    "        # Batch Preprocessing\n",
    "        comments_tensor = torch.zeros((len(comments), len(max(comments, key=len)))).long()\n",
    "        for i, comment in enumerate(comments):\n",
    "            tokenized = ['<SOS>'] + comment.split() + ['<EOS>']\n",
    "            comments_tensor[i, :len(tokenized)] = torch.tensor([word2idx[word] for word in tokenized])\n",
    "            print(comments_tensor.size())\n",
    "            \n",
    "        # Forward & Loss\n",
    "        predicted_comments = model(imgs, comments_tensor)\n",
    "        loss = criterion(predicted_comments.view(-1, len(vocab)), comments_tensor.view(-1))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        loop.set_description(f\"Epoch {epoch + 1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} finished with average loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864072e8-85dc-435d-9198-5b9e1f61bd24",
   "metadata": {},
   "source": [
    "## Inference & Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e881eb92-a172-479e-92e2-38f852a488e5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./data/open/test.csv')\n",
    "test_dataset = d.CustomDataset(test_data, transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "predicted_mos_list = []\n",
    "predicted_comments_list = []\n",
    "\n",
    "def greedy_decode(model, image, max_length=50):\n",
    "    image = image.unsqueeze(0)\n",
    "    mos, _ = model(image)\n",
    "    output_sentence = []\n",
    "    \n",
    "    # 시작 토큰 설정\n",
    "    current_token = torch.tensor([word2idx['<SOS>']])\n",
    "    hidden = None\n",
    "    features = model.cnn(image).view(image.size(0), -1)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        embeddings = model.embedding(current_token).unsqueeze(0)\n",
    "        combined = torch.cat([features.unsqueeze(1), embeddings], dim=2)\n",
    "        out, hidden = model.lstm(combined, hidden)\n",
    "        \n",
    "        output = model.fc(out.squeeze(0))\n",
    "        _, current_token = torch.max(output, dim=1)\n",
    "\n",
    "        # <EOS> 토큰에 도달하면 멈춤\n",
    "        if current_token.item() == word2idx['<EOS>']:\n",
    "            break\n",
    "\n",
    "        # <SOS> 또는 <PAD> 토큰은 생성한 캡션에 추가하지 않음\n",
    "        if current_token.item() not in [word2idx['<SOS>'], word2idx['<PAD>']]:\n",
    "            output_sentence.append(idx2word[current_token.item()])\n",
    "     \n",
    "    return mos.item(), ' '.join(output_sentence)\n",
    "\n",
    "# 추론 과정\n",
    "with torch.no_grad():\n",
    "    for imgs, _, _ in tqdm(test_loader):\n",
    "        for img in imgs:\n",
    "            img = img.float()\n",
    "            mos, caption = greedy_decode(model, img)\n",
    "            predicted_mos_list.append(mos)\n",
    "            predicted_comments_list.append(caption)\n",
    "\n",
    "# 결과 저장\n",
    "result_df = pd.DataFrame({\n",
    "    'img_name': test_data['img_name'],\n",
    "    'mos': predicted_mos_list,\n",
    "    'comments': predicted_comments_list  # 캡션 부분은 위에서 생성한 것을 사용\n",
    "})\n",
    "\n",
    "# 예측 결과에 NaN이 있다면, 제출 시 오류가 발생하므로 후처리 진행 (sample_submission.csv과 동일하게)\n",
    "result_df['comments'] = result_df['comments'].fillna('Nice Image.')\n",
    "result_df.to_csv('submit.csv', index=False)\n",
    "\n",
    "print(\"Inference completed and results saved to submit.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.14 (NGC 22.12/Python 3.8) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
